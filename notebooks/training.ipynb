{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q grad-cam==1.4.3\n",
    "!pip install -q wandb\n",
    "!pip install -q segmentation_models_pytorch\n",
    "!pip install -q torchattacks\n",
    "!pip install -q monai\n",
    "!pip install -q torchsummary\n",
    "\n",
    "# from kaggle_datasets import KaggleDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import albumentations as A\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "# import torchsummary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, shutil, time, os\n",
    "\n",
    "import sklearn\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import albumentations as A\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from skimage import color\n",
    "from IPython import display as ipd\n",
    "\n",
    "import scipy\n",
    "import pdb\n",
    "import gc\n",
    "\n",
    "import torchattacks\n",
    "import monai\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "\n",
    "from torch.cuda import amp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'lr':3e-4,\n",
    "    'shape':(224, 224),\n",
    "\n",
    "}\n",
    "TRAIN = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def seed_everything(seed=44):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def clear_cache():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_DIR = '../input/uw-madison-gi-tract-image-segmentation'\n",
    "\n",
    "# Open the training dataframe and display the initial dataframe\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "all_train_images = glob(os.path.join(TRAIN_DIR, \"**\", \"*.png\"), recursive=True)\n",
    "\n",
    "\n",
    "def get_filepath_from_partial_identifier(_ident, file_list):\n",
    "    return [x for x in file_list if _ident in x][0]\n",
    "\n",
    "def df_preprocessing(df, globbed_file_list, is_test=False):\n",
    "    \"\"\" The preprocessing steps applied to get column information \"\"\"\n",
    "    # 1. Get Case-ID as a column (str and int)\n",
    "    df[\"case_id_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n",
    "    df[\"case_id\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n",
    "\n",
    "    # 2. Get Day as a column\n",
    "    df[\"day_num_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n",
    "    df[\"day_num\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n",
    "\n",
    "    # 3. Get Slice Identifier as a column\n",
    "    df[\"slice_id\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n",
    "\n",
    "    # 4. Get full file paths for the representative scans\n",
    "    df[\"_partial_ident\"] = (globbed_file_list[0].rsplit(\"/\", 4)[0]+\"/\"+ # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n",
    "                           df[\"case_id_str\"]+\"/\"+ # .../case###/\n",
    "                           df[\"case_id_str\"]+\"_\"+df[\"day_num_str\"]+ # .../case###_day##/\n",
    "                           \"/scans/\"+df[\"slice_id\"]) # .../slice_####\n",
    "    _tmp_merge_df = pd.DataFrame({\"_partial_ident\":[x.rsplit(\"_\",4)[0] for x in globbed_file_list], \"f_path\":globbed_file_list})\n",
    "    df = df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n",
    "\n",
    "    # 5. Get slice dimensions from filepath (int in pixels)\n",
    "    df[\"slice_h\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n",
    "    df[\"slice_w\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n",
    "\n",
    "    # 6. Pixel spacing from filepath (float in mm)\n",
    "    df[\"px_spacing_h\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[3]))\n",
    "    df[\"px_spacing_w\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[4]))\n",
    "\n",
    "    if not is_test:\n",
    "        # 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\n",
    "        l_bowel_df = df[df[\"class\"]==\"large_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"lb_seg_rle\"})\n",
    "        s_bowel_df = df[df[\"class\"]==\"small_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"sb_seg_rle\"})\n",
    "        stomach_df = df[df[\"class\"]==\"stomach\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"st_seg_rle\"})\n",
    "        df = df.merge(l_bowel_df, on=\"id\", how=\"left\")\n",
    "        df = df.merge(s_bowel_df, on=\"id\", how=\"left\")\n",
    "        df = df.merge(stomach_df, on=\"id\", how=\"left\")\n",
    "        df = df.drop_duplicates(subset=[\"id\",]).reset_index(drop=True)\n",
    "        df[\"lb_seg_flag\"] = df[\"lb_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "        df[\"sb_seg_flag\"] = df[\"sb_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "        df[\"st_seg_flag\"] = df[\"st_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "        df[\"n_segs\"] = df[\"lb_seg_flag\"].astype(int)+df[\"sb_seg_flag\"].astype(int)+df[\"st_seg_flag\"].astype(int)\n",
    "\n",
    "    # 8. Reorder columns to the a new ordering (drops class and segmentation as no longer necessary)\n",
    "    new_col_order = [\"id\", \"f_path\", \"n_segs\",\n",
    "                     \"lb_seg_rle\", \"lb_seg_flag\",\n",
    "                     \"sb_seg_rle\", \"sb_seg_flag\",\n",
    "                     \"st_seg_rle\", \"st_seg_flag\",\n",
    "                     \"slice_h\", \"slice_w\", \"px_spacing_h\",\n",
    "                     \"px_spacing_w\", \"case_id_str\", \"case_id\",\n",
    "                     \"day_num_str\", \"day_num\", \"slice_id\",]\n",
    "    if is_test: new_col_order.insert(1, \"class\")\n",
    "    new_col_order = [_c for _c in new_col_order if _c in df.columns]\n",
    "    df = df[new_col_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# all_test_images = glob(os.path.join(TEST_DIR, \"**\", \"*.png\"), recursive=True)\n",
    "\n",
    "train_df = df_preprocessing(train_df, all_train_images)\n",
    "\n",
    "df = pd.read_csv(f'../input/uw-madison-gi-tract-image-segmentation/train.csv')\n",
    "df['segmentation'] = df.segmentation.fillna('')\n",
    "df['rle_len'] = df.segmentation.map(len) # length of each rle mask\n",
    "\n",
    "df2 = df.groupby(['id'])['segmentation'].agg(list).to_frame().reset_index() # rle list of each id\n",
    "df2 = df2.merge(df.groupby(['id'])['rle_len'].agg(sum).to_frame().reset_index()) # total length of all rles of each id\n",
    "df = df.drop(columns=['segmentation', 'class', 'rle_len'])\n",
    "df = df.groupby(['id']).head(1).reset_index(drop=True)\n",
    "df = df.merge(df2, on=['id'])\n",
    "df['empty'] = (df.rle_len==0) # empty masks\n",
    "\n",
    "# 1. Get Case-ID as a column (str and int)\n",
    "df[\"case_id_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n",
    "df[\"case_id\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n",
    "\n",
    "# 2. Get Day as a column\n",
    "df[\"day_num_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n",
    "df[\"day_num\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n",
    "\n",
    "# 3. Get Slice Identifier as a column\n",
    "df[\"slice_id\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n",
    "\n",
    "# 4. Get full file paths for the representative scans\n",
    "df[\"_partial_ident\"] = (all_train_images[0].rsplit(\"/\", 4)[0]+\"/\"+ # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n",
    "                       df[\"case_id_str\"]+\"/\"+ # .../case###/\n",
    "                       df[\"case_id_str\"]+\"_\"+df[\"day_num_str\"]+ # .../case###_day##/\n",
    "                       \"/scans/\"+df[\"slice_id\"]) # .../slice_####\n",
    "_tmp_merge_df = pd.DataFrame({\"_partial_ident\":[x.rsplit(\"_\",4)[0] for x in all_train_images], \"f_path\":all_train_images})\n",
    "df = df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n",
    "\n",
    "# 5. Get slice dimensions from filepath (int in pixels)\n",
    "df[\"slice_h\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n",
    "df[\"slice_w\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n",
    "\n",
    "df.rename(columns={\n",
    "    'f_path':'path',\n",
    "    'slice_h':'img_height',\n",
    "    'slice_w':'img_width',\n",
    "    'case_id':'case',\n",
    "    'day_num':'day'\n",
    "}, inplace=True)\n",
    "\n",
    "df['slice'] = df['slice_id'].apply(lambda a : int(a.split('_')[1]))\n",
    "\n",
    "df.drop(columns=['slice_id', 'case_id_str', 'day_num_str'], inplace=True)\n",
    "if TRAIN:\n",
    "    fault1 = 'case7_day0'\n",
    "    fault2 = 'case81_day30'\n",
    "    df = df[~df['id'].str.contains(fault1) & ~df['id'].str.contains(fault2)].reset_index(drop=True)\n",
    "\n",
    "df['lb'] = df['segmentation'].map(lambda a: a[0] if a[0] != '' else '')\n",
    "df['sb'] = df['segmentation'].map(lambda a: a[1] if a[1] != '' else '')\n",
    "df['st'] = df['segmentation'].map(lambda a: a[2] if a[2] != '' else '') # I know it's stupid..\n",
    "\n",
    "df['classes'] = df['segmentation'].map(lambda a: [(a[0] != '') + 0, (a[1] != '') + 0, (a[2] != '') + 0 ])\n",
    "np.random.seed(80)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/paulorzp/rle-functions-run-length-encode-decode\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "def rle_decode(mask_rle, wid, hei):\n",
    "    shape = (wid, hei)\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "\n",
    "def img_read(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_ANYDEPTH)\n",
    "    return img\n",
    "\n",
    "\n",
    "class Dataset2D(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_sub, train=True):\n",
    "        self.train = train\n",
    "\n",
    "        self.paths = np.array(df_sub['path'])\n",
    "        self.rles = np.array(df_sub['segmentation'])\n",
    "        self.classes = np.array(df_sub['classes'])\n",
    "        self.wid = np.array(df_sub['img_width'])\n",
    "        self.hei = np.array(df_sub['img_height'])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def transform(self, img, mask):\n",
    "        trans = A.Compose([\n",
    "#             A.ToFloat(max_value=65535.0), # essential because albu requires 32 bits!!! ONLY THIS can force it work with 16 bits!!\n",
    "\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "\n",
    "\n",
    "            A.ShiftScaleRotate(\n",
    "                scale_limit=0.12,  # 0\n",
    "                shift_limit=0.02,  # 0.05\n",
    "                rotate_limit=15,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                value=(1,1,1),\n",
    "                always_apply=True,\n",
    "                p=1,\n",
    "            ),\n",
    "\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1),\n",
    "\n",
    "            # A.OneOf([\n",
    "            #         A.ElasticTransform(\n",
    "            #             alpha=1,\n",
    "            #             sigma=25,\n",
    "            #             always_apply=True,\n",
    "            #         ),\n",
    "            #         A.GridDistortion(\n",
    "            #             always_apply=True,\n",
    "            #         ),\n",
    "            #         A.OpticalDistortion(\n",
    "            #             distort_limit=0.05,\n",
    "            #             shift_limit=0.05,\n",
    "            #             always_apply=True,\n",
    "            #         ),\n",
    "            #     ], p=1\n",
    "            # ),\n",
    "        ])\n",
    "        return trans(image=img, mask=mask)\n",
    "\n",
    "\n",
    "    def data_prep_aug(self, img, mask, classes):\n",
    "        shape = CFG['shape']\n",
    "        img = (cv2.resize(img, shape, interpolation=cv2.INTER_AREA) / img.max()).astype('float32')\n",
    "        mask = cv2.resize(mask, shape, interpolation=cv2.INTER_AREA).astype('float32')\n",
    "\n",
    "        if self.train:\n",
    "            trans = self.transform(img, mask)\n",
    "            img = trans['image'].reshape((1, shape[0], shape[1]))\n",
    "            mask = trans['mask']\n",
    "\n",
    "#         # normalize\n",
    "#         img = (img - img.min()) / (img.max() - img.min())\n",
    "        blank_img = np.zeros((shape[0], shape[1], 3))\n",
    "        blank_img[:, :, 0] = img\n",
    "        blank_img[:, :, 1] = img\n",
    "        blank_img[:, :, 2] = img\n",
    "        img = blank_img.transpose(2,1,0)\n",
    "\n",
    "#         plt.imshow(img.reshape(256, 256, 3))\n",
    "#         plt.pause(1)\n",
    "\n",
    "#         mask_final = np.zeros((len(classes), shape[0], shape[1]))\n",
    "#         for i in range(len(classes)):\n",
    "#             mask_final[i, :, :] = mask[:,:,i]\n",
    "        mask = mask.transpose(2,1,0)\n",
    "\n",
    "        return torch.tensor(img, dtype=torch.float16, device=device), torch.tensor(mask, dtype=torch.float16, device=device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = img_read(self.paths[idx])\n",
    "\n",
    "        blank_mask = np.zeros((self.wid[idx],  self.hei[idx], 3))\n",
    "        blank_mask[:, :, 0] = rle_decode(self.rles[idx][0], self.wid[idx], self.hei[idx])\n",
    "        blank_mask[:, :, 1] = rle_decode(self.rles[idx][1], self.wid[idx], self.hei[idx])\n",
    "        blank_mask[:, :, 2] = rle_decode(self.rles[idx][2], self.wid[idx], self.hei[idx])\n",
    "\n",
    "        # data preprocessing and augmentation\n",
    "        img, masks = self.data_prep_aug(img, blank_mask, self.classes[idx])\n",
    "\n",
    "        return img, masks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # CHECK INPUT CORRECTNESS\n",
    "# train_ds = Dataset2D(df, train=True)\n",
    "# train_ds_loader = torch.utils.data.DataLoader(train_ds, batch_size=1, num_workers=0)\n",
    "\n",
    "# for i, a in enumerate(train_ds_loader):\n",
    "#     if i < 5:\n",
    "#         print(a[1].shape)\n",
    "# #         print((a[1][0]).dtype)\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         plt.subplot(1,2,1)\n",
    "#         plt.imshow(a[0][0][0].cpu().detach().numpy().astype('float32'))\n",
    "#         plt.subplot(1,2,2)\n",
    "#         plt.imshow(a[1][0][0].cpu().detach().numpy().astype('float32'))\n",
    "#         print(a[1][0][0].cpu().detach().numpy().astype('float32').max())\n",
    "#     else:\n",
    "#         break\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def imshow(img, return_only=False, pause=False, show_axis=True):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        if len(img.shape) == 4:\n",
    "            img = img[0]\n",
    "        if img.shape[0] == 3:\n",
    "            img = img.transpose(2,1,0)\n",
    "\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        if len(img.shape) == 4:\n",
    "            img = img.cpu().detach()[0].numpy().transpose(2,1,0)\n",
    "        elif len(img.shape) == 3:\n",
    "            if img.shape[0] == 3:\n",
    "                img = img.cpu().detach().numpy().transpose(2,1,0)\n",
    "        elif len(img.shape) == 2:\n",
    "            img = img.cpu().detach().numpy()\n",
    "\n",
    "    if return_only:\n",
    "        return img\n",
    "    else:\n",
    "#         plt.figure(figsize=(5,5))\n",
    "        plt.subplots()\n",
    "        plt.imshow(img)\n",
    "        if pause:\n",
    "            plt.pause(1)\n",
    "        if not show_axis:\n",
    "            plt.axis('off')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx = 62\n",
    "\n",
    "def read(idx):\n",
    "    img = cv2.imread(df['path'][idx], cv2.IMREAD_ANYDEPTH)\n",
    "    shape = CFG['shape']\n",
    "    img = (cv2.resize(img, shape, interpolation=cv2.INTER_AREA) / img.max()).astype('float32')\n",
    "\n",
    "    blank_mask = np.zeros((df.img_width[idx],  df.img_height[idx], 3))\n",
    "    blank_mask[:, :, 0] = rle_decode(df.segmentation[idx][0], df.img_width[idx], df.img_height[idx])\n",
    "    blank_mask[:, :, 1] = rle_decode(df.segmentation[idx][1], df.img_width[idx], df.img_height[idx])\n",
    "    blank_mask[:, :, 2] = rle_decode(df.segmentation[idx][2], df.img_width[idx], df.img_height[idx])\n",
    "    mask = blank_mask\n",
    "    mask = cv2.resize(mask, shape, interpolation=cv2.INTER_AREA).astype('float32').transpose(2,1,0).reshape(1, 3, shape[0], shape[1])\n",
    "\n",
    "    blank_img = np.zeros((shape[0], shape[1], 3))\n",
    "    blank_img[:, :, 0] = img\n",
    "    blank_img[:, :, 1] = img\n",
    "    blank_img[:, :, 2] = img\n",
    "    img = blank_img.transpose(2,1,0).reshape((1, 3, shape[0], shape[1]))\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "\n",
    "def predict(idx, model, to_numpy=True, log=True):\n",
    "    img, mask = read(idx)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    mask = torch.tensor(mask, device=device,dtype=torch.float)\n",
    "    img = torch.tensor(img, device=device,dtype=torch.float)\n",
    "    pred = torch.sigmoid(model(img))\n",
    "\n",
    "    pred[pred < 0.5] = 0\n",
    "    pred[pred > 0.5] = 1\n",
    "    pred[pred == 0.5] = 1\n",
    "\n",
    "    if log:\n",
    "        dl = monai.losses.DiceLoss()(mask, pred)\n",
    "        print((1-dl.cpu().detach().numpy()))\n",
    "\n",
    "\n",
    "    if to_numpy:\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "        img = img.cpu().detach().numpy()\n",
    "        mask = mask.cpu().detach().numpy()\n",
    "\n",
    "    return img, mask, pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def FGSM_attack(model, img, mask, eps=0.007, loss=monai.losses.DiceFocalLoss(sigmoid=True)):\n",
    "\n",
    "    if isinstance(img, np.ndarray):\n",
    "        img = torch.tensor(img, device=device, dtype=torch.float)\n",
    "\n",
    "    if isinstance(mask, np.ndarray):\n",
    "        mask = torch.tensor(mask, device=device, dtype=torch.float)\n",
    "\n",
    "    img.requires_grad = True\n",
    "    output = (model(img))\n",
    "\n",
    "    loss = loss(mask, output)\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    data_grad = img.grad.data\n",
    "\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = (1-eps) * img + eps * sign_data_grad\n",
    "\n",
    "    return torch.clamp(perturbed_image, 0, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "USE_WANDB = False\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    from wandb.keras import WandbCallback\n",
    "    secret_value = 'UR WANDB KEY!'\n",
    "    wandb.login(key=secret_value)\n",
    "\n",
    "    # wandb.init(project='unet_tract_tumor')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# VGG13\n",
    "activation = None\n",
    "model_vgg = smp.Unet(\n",
    "    encoder_weights=None,\n",
    "    encoder_name='vgg13',\n",
    "    decoder_use_batchnorm=True,\n",
    "    activation=activation,\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "model_vgg = model_vgg.to(device)\n",
    "model_vgg.load_state_dict(torch.load('../input/unetvgg/unet_vgg13_12.15epochs_lr3e4.pt'))\n",
    "print(1)\n",
    "\n",
    "# UNet ResNeXt101\n",
    "activation = None\n",
    "model_res = smp.Unet(\n",
    "    encoder_weights=None,\n",
    "    encoder_name='resnext101_32x8d',\n",
    "    decoder_use_batchnorm=True,\n",
    "    activation=activation,\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "model_res = model_res.to(device)\n",
    "model_res.load_state_dict(torch.load('../input/unet-resnext101-focaldice-1215epochs-lr3e4pt/unet_resnext101_focaldice_12.15epochs_lr3e4.pt'))\n",
    "print(2)\n",
    "\n",
    "# UNet EFFB7\n",
    "activation = None\n",
    "model_eff = smp.Unet(\n",
    "    encoder_weights=None,\n",
    "    encoder_name='efficientnet-b7',\n",
    "    decoder_use_batchnorm=True,\n",
    "    activation=activation,\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "model_eff = model_eff.to(device)\n",
    "model_eff.load_state_dict(torch.load('../input/new-unet-effb7/unet_effb7_NEW11.15.pt'))\n",
    "print(3)\n",
    "\n",
    "# OLD UNet2p EFFB7\n",
    "activation = None\n",
    "model_2p = smp.UnetPlusPlus(\n",
    "    encoder_weights=None,\n",
    "    encoder_name='efficientnet-b7',\n",
    "    decoder_use_batchnorm=True,\n",
    "    activation=activation,\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "model_2p = model_2p.to(device)\n",
    "model_2p.load_state_dict(torch.load('../input/unet2p-effb7-focaldice-1315epochs-lr3e4pt/unet2p_effb7_focaldice_13.15epochs_lr3e4.pt'))\n",
    "print('finished')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model_2p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=10)\n",
    "\n",
    "# model = monai.networks.nets.UNet(\n",
    "#     spatial_dims=2,\n",
    "#     in_channels=3,\n",
    "#     out_channels=3,\n",
    "\n",
    "#     channels=(32, 64, 128, 256, 512),\n",
    "# #     strides=(2, 2, 2, 2),\n",
    "#     num_res_units=2,\n",
    "# )\n",
    "\n",
    "epochs = 15\n",
    "train_bs = 16\n",
    "\n",
    "num_epoch_2_skip = 0\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['lr'])\n",
    "criterion = lambda y_pred, y_true : monai.losses.DiceFocalLoss(sigmoid=(activation == None))(y_pred, y_true)\n",
    "# criterion = lambda y_pred, y_true : 1 - dice_coef(y_pred, y_true)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=int(30000/train_bs*epochs)+50, eta_min=5e-6,)\n",
    "\n",
    "\n",
    "\n",
    "n_accumulate = 1\n",
    "\n",
    "for fold, (train_ind, val_ind) in tqdm(enumerate(gkf.split(df, df['empty'], groups=df['case'])), desc='Train '):\n",
    "\n",
    "    if USE_WANDB:\n",
    "        wandb.init(name = f\"PostRUNS\", project=\"unet_tract_tumor_v6\", entity=\"kagglers\",\n",
    "                    config = CFG, save_code = False, group = \"UNet V3_2\",\n",
    "                    notes = \"with some da, lr=2e-3\")\n",
    "        wandb.watch(model, log='all')\n",
    "\n",
    "\n",
    "    best_val_dice = -1\n",
    "    for epoch in range(1, epochs+1):\n",
    "        clear_cache()\n",
    "        seed_everything(44)\n",
    "        val_dice = 0\n",
    "        if epoch <= num_epoch_2_skip:\n",
    "            continue\n",
    "\n",
    "        print(f'Epoch {epoch}/{epochs}', end='')\n",
    "\n",
    "        model.train()\n",
    "        scaler = torch.cuda.amp.GradScaler(init_scale=65536.0)\n",
    "\n",
    "        dataset_size = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        train_ds = Dataset2D(df.iloc[train_ind], train=True)\n",
    "        train_ds_loader = torch.utils.data.DataLoader(train_ds, batch_size=train_bs)\n",
    "\n",
    "        val_ds = Dataset2D(df.iloc[val_ind], train=False)\n",
    "        val_ds_loader = torch.utils.data.DataLoader(val_ds, batch_size=8)\n",
    "\n",
    "\n",
    "        # ========================================\n",
    "        # TRAINING\n",
    "        # ========================================\n",
    "\n",
    "        pbar = tqdm(enumerate(train_ds_loader), total=len(train_ds_loader), desc='Train ')\n",
    "\n",
    "        for step, (images, masks) in pbar:\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            with amp.autocast(enabled=True):\n",
    "                y_pred = model(images)\n",
    "                loss   = criterion(y_pred, masks)\n",
    "\n",
    "            (scaler.scale(loss)/n_accumulate).backward()\n",
    "\n",
    "            if (step + 1) % n_accumulate == 0:\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            running_loss += (loss.item() * batch_size)\n",
    "            current_loss = (loss.item())\n",
    "            dataset_size += batch_size\n",
    "\n",
    "            epoch_loss = running_loss / dataset_size\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "            if np.isnan(epoch_loss):\n",
    "                print('NAN LOSS')\n",
    "                break\n",
    "\n",
    "            if USE_WANDB:\n",
    "                wandb.log({\n",
    "                    'train_current_loss':current_loss,\n",
    "                    'lr':current_lr,\n",
    "                })\n",
    "\n",
    "            pbar.set_postfix(\n",
    "                train_loss=f'{epoch_loss:0.4f}',\n",
    "                current_loss=f'{current_loss:0.5f}',\n",
    "                lr=f'{current_lr:0.6f}',\n",
    "            )\n",
    "        if USE_WANDB:\n",
    "            wandb.log({\n",
    "                'train_epoch_loss':epoch_loss,\n",
    "            })\n",
    "        torch.save(model.state_dict(), 'ptunetbaseline_v2.pt')\n",
    "\n",
    "\n",
    "        clear_cache()\n",
    "\n",
    "\n",
    "        # ========================================\n",
    "        # Validation\n",
    "        # ========================================\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        dataset_size = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(enumerate(val_ds_loader), total=len(val_ds_loader), desc='Valid ')\n",
    "\n",
    "        for step, (images, masks) in pbar:\n",
    "            images = images.to(dtype=torch.float, device=device)\n",
    "            masks = masks.to(dtype=torch.float, device=device)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            y_pred  = model(images)\n",
    "            loss    = criterion(y_pred, masks)\n",
    "\n",
    "            running_loss += (loss.item() * batch_size)\n",
    "            dataset_size += batch_size\n",
    "\n",
    "            epoch_loss = running_loss / dataset_size\n",
    "\n",
    "            y_pred = nn.Sigmoid()(y_pred)\n",
    "\n",
    "            y_pred[y_pred < 0.5] = 0\n",
    "            y_pred[y_pred > 0.5] = 1\n",
    "\n",
    "            dl = monai.losses.DiceLoss()(masks, y_pred)\n",
    "            val_dice += (1-dl.cpu().detach().numpy())\n",
    "#             val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n",
    "\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            if USE_WANDB:\n",
    "                wandb.log({\n",
    "                    'running_valid_loss':epoch_loss,\n",
    "                })\n",
    "            pbar.set_postfix(\n",
    "                valid_loss=f'{epoch_loss:0.4f}',\n",
    "                dice_acc=f'{val_dice}'\n",
    "            )\n",
    "\n",
    "        if USE_WANDB:\n",
    "            wandb.log({\n",
    "                'val_dice':val_dice,\n",
    "                'valid_loss':epoch_loss,\n",
    "            })\n",
    "\n",
    "        if val_dice > best_val_dice:\n",
    "            best_val_dice = val_dice\n",
    "            print('saving...')\n",
    "            torch.save(model.state_dict(), f'ptunetbaseline_v2_{epoch}.pt')\n",
    "\n",
    "    if USE_WANDB:\n",
    "        wandb.finish()\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poisoned_val = 0\n",
    "poisoned_val_counts = 0\n",
    "model.eval()\n",
    "\n",
    "for a in tqdm(val_ind):\n",
    "#     img, mask, pred = predict(a, model=model_vgg, to_numpy=False, log=False)\n",
    "    img, mask = read(a)\n",
    "    if mask.max() != 0:\n",
    "        mask = torch.tensor(mask, device=device, dtype=torch.float)\n",
    "\n",
    "        pred = torch.sigmoid(model(torch.tensor(img, device=device, dtype=torch.float)))\n",
    "        pred[pred > 0.5] = 1\n",
    "        pred[pred == 0.5] = 1\n",
    "        pred[pred < 0.5] = 0\n",
    "\n",
    "\n",
    "        dl = monai.losses.DiceLoss()(mask, pred)\n",
    "        poisoned_val += (1-dl.cpu().detach().numpy())\n",
    "        poisoned_val_counts += 1\n",
    "\n",
    "# #         -- --- --- , loss=torch.nn.BCEWithLogitsLoss()\n",
    "#         poisoned_img = FGSM_attack(model=model, img=img, mask=mask, eps=0.009)\n",
    "#         poisoned_img[poisoned_img > 1] = 1\n",
    "#\n",
    "#         pred = torch.sigmoid(model(poisoned_img))\n",
    "#         mask = torch.tensor(mask, device=device, dtype=torch.float, )\n",
    "#\n",
    "#         pred[pred > 0.5] = 1\n",
    "#         pred[pred == 0.5] = 1\n",
    "#         pred[pred < 0.5] = 0\n",
    "#\n",
    "#         dl = monai.losses.DiceLoss()(mask, pred)\n",
    "#         poisoned_val += (1-dl.cpu().detach().numpy())\n",
    "#         poisoned_val_counts += 1\n",
    "# #     else:\n",
    "# #         pred = torch.sigmoid(model(torch.tensor(img, device=device, dtype=torch.float)))\n",
    "# #         pred[pred > 0.5] = 1\n",
    "# #         pred[pred == 0.5] = 1\n",
    "# #         pred[pred < 0.5] = 0\n",
    "# #         if torch.sum(pred) != 0:\n",
    "# #             print(a)\n",
    "# # #             imshow(pred)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poisoned_val / poisoned_val_counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}