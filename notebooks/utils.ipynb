{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q grad-cam==1.4.3\n",
    "!pip install -q wandb\n",
    "!pip install -q segmentation_models_pytorch\n",
    "!pip install -q torchattacks\n",
    "!pip install -q monai\n",
    "!pip install -q torchsummary\n",
    "\n",
    "# from kaggle_datasets import KaggleDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import albumentations as A\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "# import torchsummary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, shutil, time, os\n",
    "\n",
    "import sklearn\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import albumentations as A\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from skimage import color\n",
    "from IPython import display as ipd\n",
    "\n",
    "import scipy\n",
    "import pdb\n",
    "import gc\n",
    "\n",
    "import torchattacks\n",
    "import monai\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "\n",
    "from torch.cuda import amp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'lr':3e-4,\n",
    "    'shape':(224, 224),\n",
    "\n",
    "}\n",
    "TRAIN = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def seed_everything(seed=44):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def clear_cache():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_DIR = '../input/uw-madison-gi-tract-image-segmentation'\n",
    "\n",
    "# Open the training dataframe and display the initial dataframe\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "all_train_images = glob(os.path.join(TRAIN_DIR, \"**\", \"*.png\"), recursive=True)\n",
    "\n",
    "\n",
    "def get_filepath_from_partial_identifier(_ident, file_list):\n",
    "    return [x for x in file_list if _ident in x][0]\n",
    "\n",
    "def df_preprocessing(df, globbed_file_list, is_test=False):\n",
    "    \"\"\" The preprocessing steps applied to get column information \"\"\"\n",
    "    # 1. Get Case-ID as a column (str and int)\n",
    "    df[\"case_id_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n",
    "    df[\"case_id\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n",
    "\n",
    "    # 2. Get Day as a column\n",
    "    df[\"day_num_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n",
    "    df[\"day_num\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n",
    "\n",
    "    # 3. Get Slice Identifier as a column\n",
    "    df[\"slice_id\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n",
    "\n",
    "    # 4. Get full file paths for the representative scans\n",
    "    df[\"_partial_ident\"] = (globbed_file_list[0].rsplit(\"/\", 4)[0]+\"/\"+ # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n",
    "                           df[\"case_id_str\"]+\"/\"+ # .../case###/\n",
    "                           df[\"case_id_str\"]+\"_\"+df[\"day_num_str\"]+ # .../case###_day##/\n",
    "                           \"/scans/\"+df[\"slice_id\"]) # .../slice_####\n",
    "    _tmp_merge_df = pd.DataFrame({\"_partial_ident\":[x.rsplit(\"_\",4)[0] for x in globbed_file_list], \"f_path\":globbed_file_list})\n",
    "    df = df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n",
    "\n",
    "    # 5. Get slice dimensions from filepath (int in pixels)\n",
    "    df[\"slice_h\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n",
    "    df[\"slice_w\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n",
    "\n",
    "    # 6. Pixel spacing from filepath (float in mm)\n",
    "    df[\"px_spacing_h\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[3]))\n",
    "    df[\"px_spacing_w\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[4]))\n",
    "\n",
    "    if not is_test:\n",
    "        # 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\n",
    "        l_bowel_df = df[df[\"class\"]==\"large_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"lb_seg_rle\"})\n",
    "        s_bowel_df = df[df[\"class\"]==\"small_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"sb_seg_rle\"})\n",
    "        stomach_df = df[df[\"class\"]==\"stomach\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"st_seg_rle\"})\n",
    "        df = df.merge(l_bowel_df, on=\"id\", how=\"left\")\n",
    "        df = df.merge(s_bowel_df, on=\"id\", how=\"left\")\n",
    "        df = df.merge(stomach_df, on=\"id\", how=\"left\")\n",
    "        df = df.drop_duplicates(subset=[\"id\",]).reset_index(drop=True)\n",
    "        df[\"lb_seg_flag\"] = df[\"lb_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "        df[\"sb_seg_flag\"] = df[\"sb_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "        df[\"st_seg_flag\"] = df[\"st_seg_rle\"].apply(lambda x: not pd.isna(x))\n",
    "        df[\"n_segs\"] = df[\"lb_seg_flag\"].astype(int)+df[\"sb_seg_flag\"].astype(int)+df[\"st_seg_flag\"].astype(int)\n",
    "\n",
    "    # 8. Reorder columns to the a new ordering (drops class and segmentation as no longer necessary)\n",
    "    new_col_order = [\"id\", \"f_path\", \"n_segs\",\n",
    "                     \"lb_seg_rle\", \"lb_seg_flag\",\n",
    "                     \"sb_seg_rle\", \"sb_seg_flag\",\n",
    "                     \"st_seg_rle\", \"st_seg_flag\",\n",
    "                     \"slice_h\", \"slice_w\", \"px_spacing_h\",\n",
    "                     \"px_spacing_w\", \"case_id_str\", \"case_id\",\n",
    "                     \"day_num_str\", \"day_num\", \"slice_id\",]\n",
    "    if is_test: new_col_order.insert(1, \"class\")\n",
    "    new_col_order = [_c for _c in new_col_order if _c in df.columns]\n",
    "    df = df[new_col_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# all_test_images = glob(os.path.join(TEST_DIR, \"**\", \"*.png\"), recursive=True)\n",
    "\n",
    "train_df = df_preprocessing(train_df, all_train_images)\n",
    "\n",
    "df = pd.read_csv(f'../input/uw-madison-gi-tract-image-segmentation/train.csv')\n",
    "df['segmentation'] = df.segmentation.fillna('')\n",
    "df['rle_len'] = df.segmentation.map(len) # length of each rle mask\n",
    "\n",
    "df2 = df.groupby(['id'])['segmentation'].agg(list).to_frame().reset_index() # rle list of each id\n",
    "df2 = df2.merge(df.groupby(['id'])['rle_len'].agg(sum).to_frame().reset_index()) # total length of all rles of each id\n",
    "df = df.drop(columns=['segmentation', 'class', 'rle_len'])\n",
    "df = df.groupby(['id']).head(1).reset_index(drop=True)\n",
    "df = df.merge(df2, on=['id'])\n",
    "df['empty'] = (df.rle_len==0) # empty masks\n",
    "\n",
    "# 1. Get Case-ID as a column (str and int)\n",
    "df[\"case_id_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n",
    "df[\"case_id\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n",
    "\n",
    "# 2. Get Day as a column\n",
    "df[\"day_num_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n",
    "df[\"day_num\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n",
    "\n",
    "# 3. Get Slice Identifier as a column\n",
    "df[\"slice_id\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n",
    "\n",
    "# 4. Get full file paths for the representative scans\n",
    "df[\"_partial_ident\"] = (all_train_images[0].rsplit(\"/\", 4)[0]+\"/\"+ # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n",
    "                       df[\"case_id_str\"]+\"/\"+ # .../case###/\n",
    "                       df[\"case_id_str\"]+\"_\"+df[\"day_num_str\"]+ # .../case###_day##/\n",
    "                       \"/scans/\"+df[\"slice_id\"]) # .../slice_####\n",
    "_tmp_merge_df = pd.DataFrame({\"_partial_ident\":[x.rsplit(\"_\",4)[0] for x in all_train_images], \"f_path\":all_train_images})\n",
    "df = df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n",
    "\n",
    "# 5. Get slice dimensions from filepath (int in pixels)\n",
    "df[\"slice_h\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n",
    "df[\"slice_w\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n",
    "\n",
    "df.rename(columns={\n",
    "    'f_path':'path',\n",
    "    'slice_h':'img_height',\n",
    "    'slice_w':'img_width',\n",
    "    'case_id':'case',\n",
    "    'day_num':'day'\n",
    "}, inplace=True)\n",
    "\n",
    "df['slice'] = df['slice_id'].apply(lambda a : int(a.split('_')[1]))\n",
    "\n",
    "df.drop(columns=['slice_id', 'case_id_str', 'day_num_str'], inplace=True)\n",
    "if TRAIN:\n",
    "    fault1 = 'case7_day0'\n",
    "    fault2 = 'case81_day30'\n",
    "    df = df[~df['id'].str.contains(fault1) & ~df['id'].str.contains(fault2)].reset_index(drop=True)\n",
    "\n",
    "df['lb'] = df['segmentation'].map(lambda a: a[0] if a[0] != '' else '')\n",
    "df['sb'] = df['segmentation'].map(lambda a: a[1] if a[1] != '' else '')\n",
    "df['st'] = df['segmentation'].map(lambda a: a[2] if a[2] != '' else '') # I know it's stupid..\n",
    "\n",
    "df['classes'] = df['segmentation'].map(lambda a: [(a[0] != '') + 0, (a[1] != '') + 0, (a[2] != '') + 0 ])\n",
    "np.random.seed(80)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/paulorzp/rle-functions-run-length-encode-decode\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "def rle_decode(mask_rle, wid, hei):\n",
    "    shape = (wid, hei)\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "\n",
    "def img_read(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_ANYDEPTH)\n",
    "    return img\n",
    "\n",
    "\n",
    "class Dataset2D(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_sub, train=True):\n",
    "        self.train = train\n",
    "\n",
    "        self.paths = np.array(df_sub['path'])\n",
    "        self.rles = np.array(df_sub['segmentation'])\n",
    "        self.classes = np.array(df_sub['classes'])\n",
    "        self.wid = np.array(df_sub['img_width'])\n",
    "        self.hei = np.array(df_sub['img_height'])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def transform(self, img, mask):\n",
    "        trans = A.Compose([\n",
    "#             A.ToFloat(max_value=65535.0), # essential because albu requires 32 bits!!! ONLY THIS can force it work with 16 bits!!\n",
    "\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "\n",
    "\n",
    "            A.ShiftScaleRotate(\n",
    "                scale_limit=0.12,  # 0\n",
    "                shift_limit=0.02,  # 0.05\n",
    "                rotate_limit=15,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                value=(1,1,1),\n",
    "                always_apply=True,\n",
    "                p=1,\n",
    "            ),\n",
    "\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1),\n",
    "\n",
    "            # A.OneOf([\n",
    "            #         A.ElasticTransform(\n",
    "            #             alpha=1,\n",
    "            #             sigma=25,\n",
    "            #             always_apply=True,\n",
    "            #         ),\n",
    "            #         A.GridDistortion(\n",
    "            #             always_apply=True,\n",
    "            #         ),\n",
    "            #         A.OpticalDistortion(\n",
    "            #             distort_limit=0.05,\n",
    "            #             shift_limit=0.05,\n",
    "            #             always_apply=True,\n",
    "            #         ),\n",
    "            #     ], p=1\n",
    "            # ),\n",
    "        ])\n",
    "        return trans(image=img, mask=mask)\n",
    "\n",
    "\n",
    "    def data_prep_aug(self, img, mask, classes):\n",
    "        shape = CFG['shape']\n",
    "        img = (cv2.resize(img, shape, interpolation=cv2.INTER_AREA) / img.max()).astype('float32')\n",
    "        mask = cv2.resize(mask, shape, interpolation=cv2.INTER_AREA).astype('float32')\n",
    "\n",
    "        if self.train:\n",
    "            trans = self.transform(img, mask)\n",
    "            img = trans['image'].reshape((1, shape[0], shape[1]))\n",
    "            mask = trans['mask']\n",
    "\n",
    "#         # normalize\n",
    "#         img = (img - img.min()) / (img.max() - img.min())\n",
    "        blank_img = np.zeros((shape[0], shape[1], 3))\n",
    "        blank_img[:, :, 0] = img\n",
    "        blank_img[:, :, 1] = img\n",
    "        blank_img[:, :, 2] = img\n",
    "        img = blank_img.transpose(2,1,0)\n",
    "\n",
    "#         plt.imshow(img.reshape(256, 256, 3))\n",
    "#         plt.pause(1)\n",
    "\n",
    "#         mask_final = np.zeros((len(classes), shape[0], shape[1]))\n",
    "#         for i in range(len(classes)):\n",
    "#             mask_final[i, :, :] = mask[:,:,i]\n",
    "        mask = mask.transpose(2,1,0)\n",
    "\n",
    "        return torch.tensor(img, dtype=torch.float16, device=device), torch.tensor(mask, dtype=torch.float16, device=device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = img_read(self.paths[idx])\n",
    "\n",
    "        blank_mask = np.zeros((self.wid[idx],  self.hei[idx], 3))\n",
    "        blank_mask[:, :, 0] = rle_decode(self.rles[idx][0], self.wid[idx], self.hei[idx])\n",
    "        blank_mask[:, :, 1] = rle_decode(self.rles[idx][1], self.wid[idx], self.hei[idx])\n",
    "        blank_mask[:, :, 2] = rle_decode(self.rles[idx][2], self.wid[idx], self.hei[idx])\n",
    "\n",
    "        # data preprocessing and augmentation\n",
    "        img, masks = self.data_prep_aug(img, blank_mask, self.classes[idx])\n",
    "\n",
    "        return img, masks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def imshow(img, return_only=False, pause=False, show_axis=True):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        if len(img.shape) == 4:\n",
    "            img = img[0]\n",
    "        if img.shape[0] == 3:\n",
    "            img = img.transpose(2,1,0)\n",
    "\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        if len(img.shape) == 4:\n",
    "            img = img.cpu().detach()[0].numpy().transpose(2,1,0)\n",
    "        elif len(img.shape) == 3:\n",
    "            if img.shape[0] == 3:\n",
    "                img = img.cpu().detach().numpy().transpose(2,1,0)\n",
    "        elif len(img.shape) == 2:\n",
    "            img = img.cpu().detach().numpy()\n",
    "\n",
    "    if return_only:\n",
    "        return img\n",
    "    else:\n",
    "#         plt.figure(figsize=(5,5))\n",
    "        plt.subplots()\n",
    "        plt.imshow(img)\n",
    "        if pause:\n",
    "            plt.pause(1)\n",
    "        if not show_axis:\n",
    "            plt.axis('off')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx = 62\n",
    "\n",
    "def read(idx):\n",
    "    img = cv2.imread(df['path'][idx], cv2.IMREAD_ANYDEPTH)\n",
    "    shape = CFG['shape']\n",
    "    img = (cv2.resize(img, shape, interpolation=cv2.INTER_AREA) / img.max()).astype('float32')\n",
    "\n",
    "    blank_mask = np.zeros((df.img_width[idx],  df.img_height[idx], 3))\n",
    "    blank_mask[:, :, 0] = rle_decode(df.segmentation[idx][0], df.img_width[idx], df.img_height[idx])\n",
    "    blank_mask[:, :, 1] = rle_decode(df.segmentation[idx][1], df.img_width[idx], df.img_height[idx])\n",
    "    blank_mask[:, :, 2] = rle_decode(df.segmentation[idx][2], df.img_width[idx], df.img_height[idx])\n",
    "    mask = blank_mask\n",
    "    mask = cv2.resize(mask, shape, interpolation=cv2.INTER_AREA).astype('float32').transpose(2,1,0).reshape(1, 3, shape[0], shape[1])\n",
    "\n",
    "    blank_img = np.zeros((shape[0], shape[1], 3))\n",
    "    blank_img[:, :, 0] = img\n",
    "    blank_img[:, :, 1] = img\n",
    "    blank_img[:, :, 2] = img\n",
    "    img = blank_img.transpose(2,1,0).reshape((1, 3, shape[0], shape[1]))\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "\n",
    "def predict(idx, model, to_numpy=True, log=True):\n",
    "    img, mask = read(idx)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    mask = torch.tensor(mask, device=device,dtype=torch.float)\n",
    "    img = torch.tensor(img, device=device,dtype=torch.float)\n",
    "    pred = torch.sigmoid(model(img))\n",
    "\n",
    "    pred[pred < 0.5] = 0\n",
    "    pred[pred > 0.5] = 1\n",
    "    pred[pred == 0.5] = 1\n",
    "\n",
    "    if log:\n",
    "        dl = monai.losses.DiceLoss()(mask, pred)\n",
    "        print((1-dl.cpu().detach().numpy()))\n",
    "\n",
    "\n",
    "    if to_numpy:\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "        img = img.cpu().detach().numpy()\n",
    "        mask = mask.cpu().detach().numpy()\n",
    "\n",
    "    return img, mask, pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def FGSM_attack(model, img, mask, eps=0.007, loss=monai.losses.DiceFocalLoss(sigmoid=True)):\n",
    "\n",
    "    if isinstance(img, np.ndarray):\n",
    "        img = torch.tensor(img, device=device, dtype=torch.float)\n",
    "\n",
    "    if isinstance(mask, np.ndarray):\n",
    "        mask = torch.tensor(mask, device=device, dtype=torch.float)\n",
    "\n",
    "    img.requires_grad = True\n",
    "    output = (model(img))\n",
    "\n",
    "    loss = loss(mask, output)\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    data_grad = img.grad.data\n",
    "\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = (1-eps) * img + eps * sign_data_grad\n",
    "\n",
    "    return torch.clamp(perturbed_image, 0, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# VGG13\n",
    "activation = None\n",
    "model_vgg = smp.Unet(\n",
    "    encoder_weights=None,\n",
    "    encoder_name='vgg13',\n",
    "    decoder_use_batchnorm=True,\n",
    "    activation=activation,\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "model_vgg = model_vgg.to(device)\n",
    "model_vgg.load_state_dict(torch.load('../input/unetvgg/unet_vgg13_12.15epochs_lr3e4.pt'))\n",
    "print(1)\n",
    "\n",
    "# UNet ResNeXt101\n",
    "activation = None\n",
    "model_res = smp.Unet(\n",
    "    encoder_weights=None,\n",
    "    encoder_name='resnext101_32x8d',\n",
    "    decoder_use_batchnorm=True,\n",
    "    activation=activation,\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "model_res = model_res.to(device)\n",
    "model_res.load_state_dict(torch.load('../input/unet-resnext101-focaldice-1215epochs-lr3e4pt/unet_resnext101_focaldice_12.15epochs_lr3e4.pt'))\n",
    "print(2)\n",
    "\n",
    "# UNet EFFB7\n",
    "activation = None\n",
    "model_eff = smp.Unet(\n",
    "    encoder_weights=None,\n",
    "    encoder_name='efficientnet-b7',\n",
    "    decoder_use_batchnorm=True,\n",
    "    activation=activation,\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "model_eff = model_eff.to(device)\n",
    "model_eff.load_state_dict(torch.load('../input/new-unet-effb7/unet_effb7_NEW11.15.pt'))\n",
    "print(3)\n",
    "\n",
    "# OLD UNet2p EFFB7\n",
    "activation = None\n",
    "model_2p = smp.UnetPlusPlus(\n",
    "    encoder_weights=None,\n",
    "    encoder_name='efficientnet-b7',\n",
    "    decoder_use_batchnorm=True,\n",
    "    activation=activation,\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "model_2p = model_2p.to(device)\n",
    "model_2p.load_state_dict(torch.load('../input/unet2p-effb7-focaldice-1315epochs-lr3e4pt/unet2p_effb7_focaldice_13.15epochs_lr3e4.pt'))\n",
    "print('finished')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model_2p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_p = 0\n",
    "for a in model.parameters():\n",
    "    if a.requires_grad:\n",
    "#         print('asd')\n",
    "#     total_p += len(a.flatten())\n",
    "        total_p += a.numel()\n",
    "print(total_p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img, mask, pred = predict(62, model=model_2p)\n",
    "\n",
    "imshow(img * 0.6 + pred * 0.4, show_axis=False)\n",
    "# imshow(, show_axis=False)\n",
    "imshow(pred, show_axis=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mask = torch.tensor(mask, device=device,dtype=torch.float)\n",
    "img = torch.tensor(img, device=device,dtype=torch.float)\n",
    "pred = torch.tensor(pred, device=device,dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gradcam(c, img, mask, model):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        img = torch.tensor(img, device=device,dtype=torch.float)\n",
    "\n",
    "    if isinstance(mask, np.ndarray):\n",
    "        mask = torch.tensor(mask, device=device,dtype=torch.float)\n",
    "\n",
    "    if len(mask.shape) == 4:\n",
    "        mask = mask[0]\n",
    "\n",
    "#     if isinstance(pred, np.ndarray):\n",
    "#         pred = torch.tensor(pred, device=device,dtype=torch.float)\n",
    "\n",
    "    # list(model.decoder.blocks[4].children())\n",
    "    class SemanticSegmentationTarget:\n",
    "        def __init__(self, category, mask):\n",
    "            self.category = category\n",
    "            self.mask = mask\n",
    "\n",
    "        def __call__(self, model_output):\n",
    "            return (model_output[self.category, :, : ] * self.mask).sum()\n",
    "\n",
    "    targets = [SemanticSegmentationTarget(c, mask)]\n",
    "\n",
    "    # list(list(model.decoder.blocks.children())[0].children())[-2]\n",
    "    choices = [\n",
    "#         [list(model.decoder.blocks.children())[0]]\n",
    "        [list(model.decoder.blocks.children())[-1]]\n",
    "#         list(list(model_2p.decoder.blocks.children())[-1])\n",
    "    ]\n",
    "\n",
    "    cam = GradCAM(model=model,\n",
    "                 target_layers=choices[0],\n",
    "                 use_cuda=torch.cuda.is_available())\n",
    "    grayscale_cam = cam(input_tensor=img, targets=targets)\n",
    "\n",
    "    cam_3c = np.uint8(np.array([\n",
    "        grayscale_cam[0].T,\n",
    "        grayscale_cam[0].T,\n",
    "        grayscale_cam[0].T\n",
    "    ]).transpose(1,2,0) * 255)\n",
    "\n",
    "    jet_heatmap = cv2.cvtColor(cv2.applyColorMap((cam_3c), cv2.COLORMAP_JET), cv2.COLOR_BGR2RGB) / 255\n",
    "\n",
    "    return jet_heatmap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imshow(0.5 * gradcam(0, poisoned_img, poisoned_pred, model_eff) + 0.5 * img[0].transpose(2,1,0), show_axis=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_diff_v1(idx, model):\n",
    "    img, mask, pred = predict(idx, model=model)\n",
    "\n",
    "    img = img[0].transpose(2,1,0)\n",
    "    mask = mask[0].transpose(2,1,0)\n",
    "    pred = pred[0].transpose(2,1,0)\n",
    "\n",
    "\n",
    "    # False Positive\n",
    "    r1 = np.sum(pred - mask, axis=(2))\n",
    "\n",
    "    r1[r1 > 0.5] = 1\n",
    "    r1[r1 == 0.5] = 1\n",
    "    r1[r1 < 0.5] = 0\n",
    "\n",
    "    img[:,:,2] += r1\n",
    "    img[:,:,2][img[:,:,2] > 1] = 1\n",
    "\n",
    "    # False Negative\n",
    "    r2 = np.sum(mask - pred, axis=(2))\n",
    "\n",
    "    r2[r2 > 0.5] = 1\n",
    "    r2[r2 == 0.5] = 1\n",
    "    r2[r2 < 0.5] = 0\n",
    "\n",
    "    img[:,:,0] += r2\n",
    "    img[:,:,0][img[:,:,0] > 1] = 1\n",
    "\n",
    "    t = np.sum(mask * pred, axis=(2))\n",
    "    img[:,:,1] += t\n",
    "    img[:,:,1][img[:,:,1] > 1] = 1\n",
    "\n",
    "    return r1, r2, img\n",
    "\n",
    "# Below is the version 2 of show_diff\n",
    "def show_diff(img, mask, pred):\n",
    "\n",
    "    # False Positive\n",
    "    r1 = np.sum(pred - mask, axis=(2))\n",
    "\n",
    "    r1[r1 > 0.5] = 1\n",
    "    r1[r1 == 0.5] = 1\n",
    "    r1[r1 < 0.5] = 0\n",
    "\n",
    "    img[:,:,2] += r1\n",
    "    img[:,:,2][img[:,:,2] > 1] = 1\n",
    "\n",
    "    # False Negative\n",
    "    r2 = np.sum(mask - pred, axis=(2))\n",
    "\n",
    "    r2[r2 > 0.5] = 1\n",
    "    r2[r2 == 0.5] = 1\n",
    "    r2[r2 < 0.5] = 0\n",
    "\n",
    "    img[:,:,0] += r2\n",
    "    img[:,:,0][img[:,:,0] > 1] = 1\n",
    "\n",
    "    t = np.sum(mask * pred, axis=(2))\n",
    "    img[:,:,1] += t\n",
    "    img[:,:,1][img[:,:,1] > 1] = 1\n",
    "\n",
    "    return r1, r2, img"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test the one above\n",
    "imshow(show_diff(img[0].transpose(2,1,0), pred[0].transpose(2,1,0), poisoned_pred[0].cpu().detach().numpy().transpose(2,1,0))[2], show_axis=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model_2p\n",
    "img, mask, pred = predict(86, model=model)\n",
    "\n",
    "imshow(img * 0.6 + pred * 0.4, show_axis=False)\n",
    "# imshow(, show_axis=False)\n",
    "imshow(pred, show_axis=False)\n",
    "\n",
    "poisoned_img = FGSM_attack(model=model, img=img, mask=pred, eps=0.009, loss=monai.losses.FocalLoss(), show_pert=True)\n",
    "poisoned_pred = torch.sigmoid(model_2p(poisoned_img))\n",
    "\n",
    "poisoned_pred[poisoned_pred > 0.5] = 1\n",
    "poisoned_pred[poisoned_pred == 0.5] = 1\n",
    "poisoned_pred[poisoned_pred < 0.5] = 0\n",
    "\n",
    "imshow(poisoned_img, show_axis=False)\n",
    "imshow(poisoned_pred, show_axis=False)\n",
    "print((1 - monai.losses.DiceLoss()(torch.tensor(mask, device=device), poisoned_pred).cpu().detach().numpy()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=10)\n",
    "for fold, (train_ind, val_ind) in enumerate(gkf.split(df, df['empty'], groups=df['case'])):\n",
    "    train_ds = Dataset2D(df.iloc[train_ind], train=True)\n",
    "    train_ds_loader = torch.utils.data.DataLoader(train_ds, batch_size=64)\n",
    "\n",
    "    val_ds = Dataset2D(df.iloc[val_ind], train=False)\n",
    "    val_ds_loader = torch.utils.data.DataLoader(val_ds, batch_size=8)\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img, mask, pred = predict(86, model=model_2p)\n",
    "\n",
    "poisoned_img = FGSM_attack(model=model_2p, img=img, mask=pred, eps=0.009, loss=torch.monai.)\n",
    "poisoned_pred = torch.sigmoid(model_2p(poisoned_img))\n",
    "imshow(pred)\n",
    "imshow(poisoned_img)\n",
    "imshow(poisoned_pred)\n",
    "\n",
    "print((1 - monai.losses.DiceLoss()(torch.tensor(mask, device=device), poisoned_pred).cpu().detach().numpy()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poisoned_val = 0\n",
    "poisoned_val_counts = 0\n",
    "model.eval()\n",
    "\n",
    "for a in tqdm(val_ind):\n",
    "#     img, mask, pred = predict(a, model=model_vgg, to_numpy=False, log=False)\n",
    "    img, mask = read(a)\n",
    "    if mask.max() != 0:\n",
    "#         mask = torch.tensor(mask, device=device, dtype=torch.float)\n",
    "\n",
    "#         pred = torch.sigmoid(model(torch.tensor(img, device=device, dtype=torch.float)))\n",
    "#         pred[pred > 0.5] = 1\n",
    "#         pred[pred == 0.5] = 1\n",
    "#         pred[pred < 0.5] = 0\n",
    "\n",
    "\n",
    "#         dl = monai.losses.DiceLoss()(mask, pred)\n",
    "#         poisoned_val += (1-dl.cpu().detach().numpy())\n",
    "#         poisoned_val_counts += 1\n",
    "\n",
    "#         -- --- --- , loss=torch.nn.BCEWithLogitsLoss()\n",
    "        poisoned_img = FGSM_attack(model=model, img=img, mask=mask, eps=0.009)\n",
    "        poisoned_img[poisoned_img > 1] = 1\n",
    "\n",
    "        pred = torch.sigmoid(model(poisoned_img))\n",
    "        mask = torch.tensor(mask, device=device, dtype=torch.float, )\n",
    "\n",
    "        pred[pred > 0.5] = 1\n",
    "        pred[pred == 0.5] = 1\n",
    "        pred[pred < 0.5] = 0\n",
    "\n",
    "        dl = monai.losses.DiceLoss()(mask, pred)\n",
    "        poisoned_val += (1-dl.cpu().detach().numpy())\n",
    "        poisoned_val_counts += 1\n",
    "#     else:\n",
    "#         pred = torch.sigmoid(model(torch.tensor(img, device=device, dtype=torch.float)))\n",
    "#         pred[pred > 0.5] = 1\n",
    "#         pred[pred == 0.5] = 1\n",
    "#         pred[pred < 0.5] = 0\n",
    "#         if torch.sum(pred) != 0:\n",
    "#             print(a)\n",
    "# #             imshow(pred)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}